{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# üõ°Ô∏è Log G√∂zc√ºs√º - AI Model Eƒüitimi\n",
    "\n",
    "Bu notebook, log saldƒ±rƒ± tespiti i√ßin DistilBERT modelini eƒüitir.\n",
    "\n",
    "**Adƒ±mlar:**\n",
    "1. Veri y√ºkleme ve hazƒ±rlama\n",
    "2. Model eƒüitimi\n",
    "3. Deƒüerlendirme\n",
    "4. Model kaydetme (.pth)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# GPU kontrol√º\n",
    "!nvidia-smi"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# K√ºt√ºphaneleri kur\n",
    "!pip install -q transformers datasets torch scikit-learn"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1Ô∏è‚É£ Config"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Konfig√ºrasyon\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MAX_SEQ_LENGTH = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5\n",
    "DROPOUT = 0.3\n",
    "\n",
    "LABEL_MAP = {\n",
    "    0: \"benign\",\n",
    "    1: \"sqli\",\n",
    "    2: \"xss\",\n",
    "    3: \"path_traversal\",\n",
    "    4: \"command_injection\",\n",
    "    5: \"bruteforce\",\n",
    "    6: \"honeypot_trap\",\n",
    "    7: \"other_attack\"\n",
    "}\n",
    "LABEL_TO_ID = {v: k for k, v in LABEL_MAP.items()}\n",
    "NUM_CLASSES = len(LABEL_MAP)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2Ô∏è‚É£ Veri Y√ºkleme\n",
    "\n",
    "**Veri dosyasƒ±nƒ± y√ºkleyin:**\n",
    "- Sol men√ºden üìÅ Files'a tƒ±klayƒ±n\n",
    "- `train_data.jsonl` dosyasƒ±nƒ± s√ºr√ºkleyip bƒ±rakƒ±n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Veri y√ºkleme fonksiyonu\n",
    "def load_jsonl(file_path):\n",
    "    samples = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                try:\n",
    "                    sample = json.loads(line)\n",
    "                    if 'log' in sample and 'label' in sample:\n",
    "                        samples.append(sample)\n",
    "                except:\n",
    "                    continue\n",
    "    return samples\n",
    "\n",
    "# Veriyi y√ºkle (dosya yolunu g√ºncelleyin)\n",
    "# data = load_jsonl('/content/train_data.jsonl')\n",
    "# print(f\"Toplam √∂rnek: {len(data)}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Demo: √ñrnek veri olu≈ütur (ger√ßek veri yoksa test i√ßin)\n",
    "demo_data = [\n",
    "    {\"log\": '192.168.1.1 - - [10/Jan/2026:12:00:00] \"GET /index.html HTTP/1.1\" 200 5678', \"label\": \"benign\"},\n",
    "    {\"log\": '10.0.0.5 - - [10/Jan/2026:12:01:00] \"GET /api/users HTTP/1.1\" 200 1234', \"label\": \"benign\"},\n",
    "    {\"log\": '192.168.1.100 - - [10/Jan/2026:12:03:00] \"GET /login?user=admin\\' OR \\'1\\'=\\'1 HTTP/1.1\" 200 1234', \"label\": \"sqli\"},\n",
    "    {\"log\": '10.0.0.50 - - [10/Jan/2026:12:04:00] \"GET /product?id=1 UNION SELECT * FROM users HTTP/1.1\" 200 3000', \"label\": \"sqli\"},\n",
    "    {\"log\": '192.168.1.200 - - [10/Jan/2026:12:05:00] \"GET /search?q=<script>alert(1)</script> HTTP/1.1\" 200 500', \"label\": \"xss\"},\n",
    "    {\"log\": '10.0.0.100 - - [10/Jan/2026:12:06:00] \"GET /download?file=../../etc/passwd HTTP/1.1\" 200 1500', \"label\": \"path_traversal\"},\n",
    "    {\"log\": '172.16.0.50 - - [10/Jan/2026:12:07:00] \"GET /ping?ip=127.0.0.1;cat /etc/shadow HTTP/1.1\" 200 2000', \"label\": \"command_injection\"},\n",
    "    {\"log\": '192.168.1.150 - - [10/Jan/2026:12:08:00] \"POST /login HTTP/1.1\" 401 100', \"label\": \"bruteforce\"},\n",
    "] * 100  # Demo i√ßin √ßoƒüalt\n",
    "\n",
    "data = demo_data  # Ger√ßek veri y√ºklendiƒüinde bu satƒ±rƒ± kaldƒ±rƒ±n\n",
    "print(f\"Toplam √∂rnek: {len(data)}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3Ô∏è‚É£ Dataset & DataLoader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class LogDataset(Dataset):\n",
    "    def __init__(self, samples, tokenizer, max_length=256):\n",
    "        self.samples = samples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        log_text = sample['log']\n",
    "        label = LABEL_TO_ID.get(sample['label'].lower(), 7)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            log_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(label)\n",
    "        }\n",
    "\n",
    "# Tokenizer y√ºkle\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Train/Val split\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "print(f\"Train: {len(train_data)}, Val: {len(val_data)}\")\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = LogDataset(train_data, tokenizer, MAX_SEQ_LENGTH)\n",
    "val_dataset = LogDataset(val_data, tokenizer, MAX_SEQ_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4Ô∏è‚É£ Model Tanƒ±mƒ±"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class LogClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(768, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.classifier(self.dropout(pooled))\n",
    "\n",
    "model = LogClassifier().to(device)\n",
    "print(f\"Model parametreleri: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5Ô∏è‚É£ Eƒüitim"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Optimizer ve Scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=total_steps)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Eƒüitim d√∂ng√ºs√º\n",
    "best_val_acc = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    val_acc = correct / total\n",
    "    print(f\"üìä Epoch {epoch+1}: Loss={avg_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'log_classifier_best.pth')\n",
    "        print(f\"‚úÖ En iyi model kaydedildi! Acc: {val_acc:.4f}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6Ô∏è‚É£ Deƒüerlendirme"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# En iyi modeli y√ºkle\n",
    "model.load_state_dict(torch.load('log_classifier_best.pth'))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Rapor\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=list(LABEL_MAP.values())))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7Ô∏è‚É£ Model Kaydetme"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Final model kaydet\n",
    "torch.save(model.state_dict(), 'log_classifier.pth')\n",
    "print(\"‚úÖ Model kaydedildi: log_classifier.pth\")\n",
    "\n",
    "# ƒ∞ndirmek i√ßin\n",
    "from google.colab import files\n",
    "files.download('log_classifier.pth')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üéâ Tamamlandƒ±!\n",
    "\n",
    "**Sonraki adƒ±mlar:**\n",
    "1. `log_classifier.pth` dosyasƒ±nƒ± indir\n",
    "2. Log G√∂zc√ºs√º projesine kopyala: `ai_model/log_classifier.pth`\n",
    "3. `ajan.py`'daki API kodlarƒ±nƒ± kaldƒ±r\n",
    "4. Test et!"
   ],
   "metadata": {}
  }
 ]
}
